## NATURAL LANGUAGE PROCESSING METRICS
- **Accuracy**: The proportion of correct predictions to the total number of predictions.
- **Precision**: The proportion of true positive predictions to the total number of positive predictions.
- **Recall**: The proportion of true positive predictions to the total number of actual positive instances.
- **F1 Score**: The harmonic mean of precision and recall.
- **Confusion Matrix**: A table that describes the performance of a classification model.
- **ROC Curve**: A graphical representation of the true positive rate against the false positive rate.
- **AUC-ROC**: The area under the ROC curve.
- **Log Loss**: The negative log-likelihood of the true labels given the predicted probabilities.
- **Perplexity**: A measure of how well a probability model predicts a sample.
- **BLEU Score**: A metric for evaluating a generated sentence to a reference sentence.
- **WER**: The word error rate is a metric for evaluating the performance of a speech recognition system.
- **METEOR**: A metric for evaluating the quality of a machine-generated translation.
- **ROUGE**: A metric for evaluating the quality of a machine-generated summary.
- **CIDEr**: A metric for evaluating the quality of a machine-generated description.

## WORD EMBEDDING METRICS
- These metrics are used to evaluate the quality of word embeddings.
